{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df9c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === Upload spaCy English Model ===\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === 1. Upload ===\n",
    "def load_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# === 1. lemmatization ===\n",
    "def singularize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# === 2. Clear Concent ===\n",
    "def clean_phrase(phrase):\n",
    "    phrase = phrase.strip().lower()\n",
    "    phrase = re.sub(r'^[\\\"\\'\\d]+\\s*', '', phrase)\n",
    "    phrase = re.sub(r'\\s*\\d+\\s*', ' ', phrase)\n",
    "    phrase = re.sub(r'\\s{2,}', ' ', phrase)\n",
    "    phrase = re.sub(r'^(a |an |any |the |their |these |those )', '', phrase)\n",
    "    # print(f\"{phrase}\")\n",
    "    return phrase.strip()\n",
    "\n",
    "# === 3. Filter Unnecessary phrases ===\n",
    "def is_valid_entity(phrase):\n",
    "    phrase = phrase.strip().lower()\n",
    "    words = phrase.split()\n",
    "    if len(words) < 2 or len(phrase) > 80:\n",
    "        return False\n",
    "\n",
    "    strong_keywords = [\"number\", \"diagram\", \"notation\", \"square\", \"root\", \"property\", \"tile\", \"representation\"]\n",
    "    weak_starts = [\"a \", \"an \", \"their \"]\n",
    "    weak_concepts = {\"difference\", \"relationship\", \"area\", \"value\", \"perimeter\", \"amount\"}\n",
    "\n",
    "    if any(kw in phrase for kw in strong_keywords):\n",
    "        return True\n",
    "    if all(w in weak_concepts for w in words):\n",
    "        return False\n",
    "    if any(phrase.startswith(s) for s in weak_starts):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# === 4. Extract Entities ===\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    concepts = set()\n",
    "    theorems = set()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = clean_phrase(chunk.text)\n",
    "        if not is_valid_entity(phrase):\n",
    "            continue\n",
    "        if any(kw in phrase for kw in [\"notation\", \"property\", \"formula\", \"law\", \"diagram\", \"rule\", \"principle\", \"theorem\"]):\n",
    "            theorems.add(phrase)\n",
    "        else:\n",
    "            concepts.add(phrase)\n",
    "\n",
    "    concept_entities = [{\"name\": c, \"type\": 1} for c in sorted(concepts)]\n",
    "    theorem_entities = [{\"name\": t, \"type\": 2} for t in sorted(theorems)]\n",
    "    return concept_entities + theorem_entities\n",
    "\n",
    "# === 5. Execute ===\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"year 7-10 firstone.txt\"  \n",
    "    text = load_text(file_path)\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    df = pd.DataFrame(entities)\n",
    "    df['name'] = df['name'].apply(clean_phrase)\n",
    "    df['name'] = df['name'].apply(singularize)  \n",
    "    df = df.drop_duplicates().sort_values(by='type')\n",
    "\n",
    "    df.to_csv(\"final_entities.csv\", index=False)\n",
    "    df.to_json(\"final_entities.json\", indent=2, force_ascii=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe5923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Full JSON lines =====\n",
      "{\n",
      "\"name\":{\n",
      "\"0\":\"consecutive natural number\",\n",
      "\"1\":\"consecutive square number\",\n",
      "\"2\":\"constant second difference\",\n",
      "\"3\":\"emerge pattern\",\n",
      "\"4\":\"natural number\",\n",
      "\"5\":\"perfect square number\",\n",
      "\"6\":\"square number\",\n",
      "\"7\":\"square pattern\",\n",
      "\"8\":\"square root\",\n",
      "\"10\":\"square tile floor\",\n",
      "\"11\":\"square tile\",\n",
      "\"12\":\"tile length\",\n",
      "\"13\":\"two - digit number\",\n",
      "\"14\":\"visual representation\",\n",
      "\"15\":\"distributive property and area diagram\",\n",
      "\"16\":\"square and square root notation\"\n",
      "},\n",
      "\"type\":{\n",
      "\"0\":1,\n",
      "\"1\":1,\n",
      "\"2\":1,\n",
      "\"3\":1,\n",
      "\"4\":1,\n",
      "\"5\":1,\n",
      "\"6\":1,\n",
      "\"7\":1,\n",
      "\"8\":1,\n",
      "\"10\":1,\n",
      "\"11\":1,\n",
      "\"12\":1,\n",
      "\"13\":1,\n",
      "\"14\":1,\n",
      "\"15\":2,\n",
      "\"16\":2\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === Get entities.json  ===\n",
    "print(\"\\n===== Full JSON lines =====\")\n",
    "with open(\"final_entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8977a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split entities into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da2391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === Split entities into train and test sets ===\n",
    "txt_file = \"concepts.txt\"\n",
    "json_file = \"entities.json\"\n",
    "\n",
    "entity_list = []\n",
    "with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().rsplit(\" \", 1)\n",
    "        if len(parts) == 2 and parts[1] in [\"1\", \"2\"]:\n",
    "            label = \"KNOW\" if parts[1] == \"1\" else \"PRIN\"\n",
    "            entity_list.append({\n",
    "                \"name\": parts[0].strip().lower(),\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "# Write to JSON file\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(entity_list, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbea851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ====== 1. Open entiteis ======\n",
    "with open(\"entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entity_list = json.load(f)\n",
    "\n",
    "#  Prioritize longer entities for matching\n",
    "entity_list = sorted(entity_list, key=lambda e: -len(e[\"name\"].split()))\n",
    "\n",
    "# dictionary form (lowercase matching)\n",
    "entity_dict = {e[\"name\"].lower(): e[\"label\"] for e in entity_list}\n",
    "\n",
    "\n",
    "# ====== 2. Upload context======\n",
    "with open(\"context.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "# ====== 3. tokenize ======\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "# ====== 4. Generate Bio Tag ======\n",
    "def tag_sentence(sentence, entity_dict):\n",
    "    tokens = tokenize(sentence)\n",
    "    lowered = [t.lower() for t in tokens]\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    for ent, label in entity_dict.items():\n",
    "        ent_tokens = ent.split()\n",
    "        n = len(ent_tokens)\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            if lowered[i:i+n] == ent_tokens and tags[i] == \"O\":\n",
    "                tags[i] = f\"B-{label}\"\n",
    "                for j in range(1, n):\n",
    "                    tags[i+j] = f\"I-{label}\"\n",
    "\n",
    "    return list(zip(tokens, tags))\n",
    "\n",
    "\n",
    "# ====== 5. Write Bio ======\n",
    "with open(\"bert_ner_bio.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    for sentence in sentences:\n",
    "        token_tag_pairs = tag_sentence(sentence, entity_dict)\n",
    "        for token, tag in token_tag_pairs:\n",
    "            out.write(f\"{token}\\t{tag}\\n\")\n",
    "        out.write(\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1ec797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence： 2233\n"
     ]
    }
   ],
   "source": [
    "with open(\"context.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"Original Sentence：\", sum(1 for line in f if line.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf75ce61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO Sebtebce： 2233\n"
     ]
    }
   ],
   "source": [
    "sentence_count = 0\n",
    "with open(\"bert_ner_bio.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == \"\":\n",
    "            sentence_count += 1\n",
    "print(\"BIO Sebtebce：\", sentence_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c117c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity distribution：\n",
      "O: 86856\n",
      "B-KNOW: 19623\n",
      "I-KNOW: 8211\n",
      "B-PRIN: 849\n",
      "I-PRIN: 1141\n"
     ]
    }
   ],
   "source": [
    "#Analyze entity distribution\n",
    "from collections import Counter\n",
    "def analyze_entity_distribution(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tags = [line.strip().split(\"\\t\")[1] for line in lines if line.strip()]\n",
    "    tag_counts = Counter(tags)\n",
    "\n",
    "    return tag_counts\n",
    "# analyze_entity_distribution(\"bert_ner_bio.txt\")\n",
    "distribution = analyze_entity_distribution(\"bert_ner_bio.txt\")\n",
    "# print entity distribution\n",
    "print(\"entity distribution：\")\n",
    "for tag, count in distribution.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1efa4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已分割为 train_1.txt 和 test_2.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_data_by_sentence(input_file, train_file, test_file, train_ratio=0.75):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # separate sentences by double newlines\n",
    "    sentences = content.strip().split(\"\\n\\n\")\n",
    "\n",
    "    # shuffle sentences\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # Calculate split index\n",
    "    split_index = int(len(sentences) * train_ratio)\n",
    "\n",
    "    # Write train and test files\n",
    "    with open(train_file, \"w\", encoding=\"utf-8\") as f_train:\n",
    "        f_train.write(\"\\n\\n\".join(sentences[:split_index]) + \"\\n\")\n",
    "\n",
    "    with open(test_file, \"w\", encoding=\"utf-8\") as f_test:\n",
    "        f_test.write(\"\\n\\n\".join(sentences[split_index:]) + \"\\n\")\n",
    "\n",
    "    print(f\"数据集已分割为 {train_file} 和 {test_file}\")\n",
    "\n",
    "# 调用函数\n",
    "split_data_by_sentence(\"bert_ner_bio.txt\", \"train_1.txt\", \"test_2.txt\", train_ratio=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b8fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contribution：\n",
      "B-KNOW: 14634\n",
      "I-KNOW: 6100\n",
      "O: 64764\n",
      "B-PRIN: 681\n",
      "I-PRIN: 916\n",
      "Test dataset contribution：\n",
      "O: 22092\n",
      "B-KNOW: 4989\n",
      "I-KNOW: 2111\n",
      "B-PRIN: 168\n",
      "I-PRIN: 225\n"
     ]
    }
   ],
   "source": [
    "#同理，分析train.txt和test.txt的实体分布\n",
    "## 分析train分布\n",
    "train_distribution = analyze_entity_distribution(\"train_1.txt\")\n",
    "# 输出实体分布  \n",
    "print(\"Train dataset contribution：\")\n",
    "for tag, count in train_distribution.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "## 分析test分布\n",
    "test_distribution = analyze_entity_distribution(\"test_2.txt\")\n",
    "# 输出实体分布  \n",
    "print(\"Test dataset contribution：\")\n",
    "for tag, count in test_distribution.items():\n",
    "    print(f\"{tag}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8240d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ====== 1. generate ======\n",
    "with open(\"entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entity_list = json.load(f)\n",
    "\n",
    "# prioritize longer entities for matching\n",
    "entity_list = sorted(entity_list, key=lambda e: -len(e[\"name\"].split()))\n",
    "\n",
    "# dictionary form (lowercase matching)\n",
    "entity_dict = {e[\"name\"].lower(): e[\"label\"] for e in entity_list}\n",
    "\n",
    "\n",
    "# ====== 2.get examples======\n",
    "with open(\"year 7-10 firstone1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "# ====== 3. tokenize ======\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "# ====== 4. Gemerate semtemse=====\n",
    "def tag_sentence(sentence, entity_dict):\n",
    "    tokens = tokenize(sentence)\n",
    "    lowered = [t.lower() for t in tokens]\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    for ent, label in entity_dict.items():\n",
    "        ent_tokens = ent.split()\n",
    "        n = len(ent_tokens)\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            if lowered[i:i+n] == ent_tokens and tags[i] == \"O\":\n",
    "                tags[i] = f\"B-{label}\"\n",
    "                for j in range(1, n):\n",
    "                    tags[i+j] = f\"I-{label}\"\n",
    "\n",
    "    return list(zip(tokens, tags))\n",
    "\n",
    "\n",
    "# ====== 5. Write Bio file ======\n",
    "with open(\"bert_ner_bio1.txt\", \"w\", encoding=\"utf-8\") as out:\n",
    "    for sentence in sentences:\n",
    "        token_tag_pairs = tag_sentence(sentence, entity_dict)\n",
    "        for token, tag in token_tag_pairs:\n",
    "            out.write(f\"{token}\\t{tag}\\n\")\n",
    "        out.write(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390fbfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "describe\tO\n",
      "the\tO\n",
      "relationship\tO\n",
      "between\tO\n",
      "perfect\tB-KNOW\n",
      "square\tI-KNOW\n",
      "numbers\tI-KNOW\n",
      "and\tO\n",
      "square\tB-KNOW\n",
      "roots\tI-KNOW\n",
      ".\tO\n",
      "\n",
      "and\tO\n",
      "use\tO\n",
      "squares\tO\n",
      "of\tO\n",
      "numbers\tO\n",
      "and\tO\n",
      "square\tB-KNOW\n",
      "roots\tI-KNOW\n",
      "of\tO\n",
      "perfect\tB-KNOW\n",
      "square\tI-KNOW\n",
      "numbers\tI-KNOW\n",
      "to\tO\n",
      "solve\tO\n",
      "problems\tO\n",
      ".\tO\n",
      "\n",
      "investigating\tO\n",
      "squares\tO\n",
      "of\tO\n",
      "natural\tO\n",
      "numbers\tO\n",
      "from\tO\n",
      "one\tO\n",
      "to\tO\n",
      "20\tO\n",
      ",\tO\n",
      "and\tO\n",
      "connecting\tO\n",
      "them\tO\n",
      "to\tO\n",
      "visual\tO\n",
      "representations\tO\n",
      "such\tO\n",
      "as\tO\n",
      "dots\tO\n",
      "arranged\tO\n",
      "in\tO\n",
      "a\tO\n",
      "square\tB-KNOW\n",
      "pattern\tO\n",
      ".\tO\n",
      "\n",
      "using\tO\n",
      "the\tO\n",
      "square\tB-KNOW\n",
      "and\tO\n",
      "square\tB-KNOW\n",
      "root\tI-KNOW\n",
      "notation\tO\n",
      ",\tO\n",
      "and\tO\n",
      "the\tO\n",
      "distributive\tO\n",
      "property\tO\n",
      "and\tO\n",
      "area\tB-KNOW\n",
      "diagrams\tO\n",
      "to\tO\n",
      "calculate\tO\n",
      "the\tO\n",
      "squares\tO\n",
      "of\tO\n",
      "two\tO\n",
      "-\tO\n",
      "digit\tO\n",
      "numbers\tO\n",
      ";\tO\n",
      "for\tO\n",
      "example\tO\n",
      ",\tO\n",
      "43\tO\n",
      "^\tO\n",
      "2\tO\n",
      "=\tO\n",
      "(\tO\n",
      "40\tO\n",
      "+\tO\n",
      "3\tO\n",
      ")\tO\n",
      "^\tO\n",
      "2\tO\n",
      "=\tO\n",
      "40\tO\n",
      "^\tO\n",
      "2\tO\n",
      "+\tO\n",
      "2\tO\n",
      "×\tO\n",
      "40\tO\n",
      "×\tO\n",
      "3\tO\n",
      "+\tO\n",
      "3\tO\n",
      "^\tO\n",
      "2\tO\n",
      "=\tO\n",
      "1600\tO\n",
      "+\tO\n",
      "240\tO\n",
      "+\tO\n",
      "9\tO\n",
      "=\tO\n",
      "1849\tO\n",
      ".\tO\n",
      "\n",
      "determining\tO\n",
      "between\tO\n",
      "which\tO\n",
      "2\tO\n",
      "consecutive\tO\n",
      "natural\tO\n",
      "numbers\tO\n",
      "the\tO\n",
      "square\tB-KNOW\n",
      "root\tI-KNOW\n",
      "of\tO\n",
      "a\tO\n",
      "given\tO\n",
      "number\tB-KNOW\n",
      "lies\tO\n",
      ";\tO\n",
      "for\tO\n",
      "example\tO\n",
      ",\tO\n",
      "43\tO\n",
      "is\tO\n",
      "between\tO\n",
      "the\tO\n",
      "square\tB-KNOW\n",
      "numbers\tO\n",
      "36\tO\n",
      "and\tO\n",
      "49\tO\n",
      "so\tO\n",
      "√\tO\n",
      "43\tO\n",
      "is\tO\n",
      "between\tO\n",
      "√\tO\n",
      "36\tO\n",
      "\"\tO\n",
      "and\tO\n",
      "\"\tO\n",
      "√\tO\n",
      "49\tO\n",
      "and\tO\n",
      "therefore\tO\n",
      "between\tO\n",
      "6\tO\n",
      "and\tO\n",
      "7\tO\n",
      ".\tO\n",
      "\n",
      "generating\tO\n",
      "a\tO\n",
      "list\tO\n",
      "of\tO\n",
      "perfect\tB-KNOW\n",
      "square\tI-KNOW\n",
      "numbers\tI-KNOW\n",
      "and\tO\n",
      "describing\tO\n",
      "any\tO\n",
      "emerging\tO\n",
      "patterns\tO\n",
      ";\tO\n",
      "for\tO\n",
      "example\tO\n",
      ",\tO\n",
      "the\tO\n",
      "last\tO\n",
      "digit\tO\n",
      "of\tO\n",
      "perfect\tB-KNOW\n",
      "square\tI-KNOW\n",
      "numbers\tI-KNOW\n",
      ",\tO\n",
      "or\tO\n",
      "the\tO\n",
      "difference\tB-KNOW\n",
      "between\tO\n",
      "consecutive\tO\n",
      "square\tB-KNOW\n",
      "numbers\tO\n",
      ",\tO\n",
      "and\tO\n",
      "recognising\tO\n",
      "the\tO\n",
      "constant\tB-KNOW\n",
      "second\tO\n",
      "difference\tB-KNOW\n",
      ".\tO\n",
      "\n",
      "using\tO\n",
      "the\tO\n",
      "relationship\tO\n",
      "between\tO\n",
      "perfect\tB-KNOW\n",
      "square\tI-KNOW\n",
      "numbers\tI-KNOW\n",
      "and\tO\n",
      "their\tO\n",
      "square\tB-KNOW\n",
      "roots\tI-KNOW\n",
      "to\tO\n",
      "determine\tO\n",
      "the\tO\n",
      "perimeter\tB-KNOW\n",
      "of\tO\n",
      "a\tO\n",
      "square\tB-KNOW\n",
      "tiled\tO\n",
      "floor\tO\n",
      "using\tO\n",
      "square\tB-KNOW\n",
      "tiles\tO\n",
      ";\tO\n",
      "for\tO\n",
      "example\tO\n",
      ",\tO\n",
      "an\tO\n",
      "area\tB-KNOW\n",
      "of\tO\n",
      "floor\tO\n",
      "with\tO\n",
      "144\tO\n",
      "square\tB-KNOW\n",
      "tiles\tO\n",
      "has\tO\n",
      "a\tO\n",
      "perimeter\tB-KNOW\n",
      "of\tO\n",
      "48\tO\n",
      "tile\tO\n",
      "lengths\tO\n",
      ".\tO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('bert_ner_bio1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dc9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "describe the relationship between perfect square numbers and square roots .\n",
      "O O O O B-KNOW I-KNOW I-KNOW O B-KNOW I-KNOW O\n",
      "\n",
      "and use squares of numbers and square roots of perfect square numbers to solve problems .\n",
      "O O O O O O B-KNOW I-KNOW O B-KNOW I-KNOW I-KNOW O O O O\n",
      "\n",
      "investigating squares of natural numbers from one to 20 , and connecting them to visual representations such as dots arranged in a square pattern .\n",
      "O O O O O O O O O O O O O O O O O O O O O O B-KNOW O O\n",
      "\n",
      "using the square and square root notation , and the distributive property and area diagrams to calculate the squares of two - digit numbers ; for example , 43 ^ 2 = ( 40 + 3 ) ^ 2 = 40 ^ 2 + 2 × 40 × 3 + 3 ^ 2 = 1600 + 240 + 9 = 1849 .\n",
      "O O B-KNOW O B-KNOW I-KNOW O O O O O O O B-KNOW O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "determining between which 2 consecutive natural numbers the square root of a given number lies ; for example , 43 is between the square numbers 36 and 49 so √ 43 is between √ 36 \" and \" √ 49 and therefore between 6 and 7 .\n",
      "O O O O O O O O B-KNOW I-KNOW O O O B-KNOW O O O O O O O O O B-KNOW O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "generating a list of perfect square numbers and describing any emerging patterns ; for example , the last digit of perfect square numbers , or the difference between consecutive square numbers , and recognising the constant second difference .\n",
      "O O O O B-KNOW I-KNOW I-KNOW O O O O O O O O O O O O O B-KNOW I-KNOW I-KNOW O O O B-KNOW O O B-KNOW O O O O O B-KNOW O B-KNOW O\n",
      "\n",
      "using the relationship between perfect square numbers and their square roots to determine the perimeter of a square tiled floor using square tiles ; for example , an area of floor with 144 square tiles has a perimeter of 48 tile lengths .\n",
      "O O O O B-KNOW I-KNOW I-KNOW O O B-KNOW I-KNOW O O O B-KNOW O O B-KNOW O O O B-KNOW O O O O O O B-KNOW O O O O B-KNOW O O O B-KNOW O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "\n",
    "# 1. Initialize\n",
    "sent_words = []\n",
    "sent_tags = []\n",
    "\n",
    "# 2. Get txt file\n",
    "with open('bert_ner_bio1.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:  \n",
    "            if sent_words:\n",
    "                print(' '.join(sent_words))\n",
    "                print(' '.join(sent_tags))\n",
    "                print()  \n",
    "                sent_words = []\n",
    "                sent_tags = []\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                word, tag = parts\n",
    "                sent_words.append(word)\n",
    "                sent_tags.append(tag)\n",
    "\n",
    "# 3. Handle the last sentence\n",
    "if sent_words:\n",
    "    print(' '.join(sent_words))\n",
    "    print(' '.join(sent_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b12ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
